{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import animation, pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.linalg as linalg\n",
    "import scipy.stats as stats\n",
    "import scipy.interpolate as interp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%pylab inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Read in worm data from SharedData .npz file\n",
    "Worm = 'GFP'\n",
    "Centerline = True\n",
    "Neuron_Position = False\n",
    "Close = False\n",
    "\n",
    "worm_data = np.load('../SharedData/Worm'+Worm+'.npz')\n",
    "print('The loaded Primary npz contains the variables:\\n', np.sort([i for i in worm_data]))\n",
    "\n",
    "G_sig = 'GFP' if Worm == 'GFP' else 'GCaMP'\n",
    "\n",
    "### Import desired variables\n",
    "G_Raw = worm_data['G_Raw']\n",
    "R_Raw = worm_data['R_Raw']\n",
    "Time = worm_data['Time']\n",
    "\n",
    "\n",
    "### Import Centerline Data?\n",
    "if Centerline:\n",
    "    CLdata = np.load('../SharedData/Worm_Angles/WormAngle'+Worm+'.npz')\n",
    "    print('The loaded Centerline npz contains the variables:\\n', np.sort([i for i in CLdata]))\n",
    "    \n",
    "    CL_PCs = CLdata['proj_neural_thetas'].T\n",
    "\n",
    "\n",
    "### Import Neuron Position Data?\n",
    "if Neuron_Position:\n",
    "    if Close:\n",
    "        NPdata = np.load('../SharedData/NPos_PCA/Worm'+Worm+'_NPosPCA_Close.npz')\n",
    "        print(\"Close neurons with no tail was loaded\")\n",
    "    else:\n",
    "        NPdata = np.load('../SharedData/NPos_PCA/Worm'+Worm+'_NPosPCA.npz')\n",
    "    print('The loaded Neuron Position npz contains the variables:\\n', np.sort([i for i in NPdata]))\n",
    "    \n",
    "    NP_PCs = NPdata['NP_PCs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Determine which time columns have NaNs pick first and last w/o any NaNs\n",
    "badcols = np.array([x|y for (x,y) in zip(np.isnan(G_Raw).any(axis=0), np.isnan(R_Raw).any(axis=0))])\n",
    "begin_col = np.arange(0,len(Time))[~badcols][0]\n",
    "end_col = np.arange(0,len(Time))[~badcols][-1] + 1\n",
    "\n",
    "### Shave off times from before first and last full column\n",
    "G_Raw = G_Raw[:,begin_col:end_col]\n",
    "R_Raw = R_Raw[:,begin_col:end_col]\n",
    "Time = Time[begin_col:end_col, 0]\n",
    "\n",
    "if Centerline: CL_PCs = CL_PCs[:,begin_col:end_col]\n",
    "if Neuron_Position: NP_PCs = NP_PCs[:,begin_col:end_col]\n",
    "    \n",
    "    \n",
    "[neuron_length, neuron_time] = np.shape(G_Raw)\n",
    "print('Neurons:', neuron_length, '\\nTime Points:', neuron_time, '\\nFrom', Time[0], 's to', Time[-1], 's')\n",
    "\n",
    "### Fill in NaNs with interpolation\n",
    "for i in np.arange(len(G_Raw)):\n",
    "    \n",
    "    g_bad = np.isnan(G_Raw[i])\n",
    "    if g_bad.any():\n",
    "        g_interp = interp.interp1d(Time[~g_bad], G_Raw[i,~g_bad], kind='cubic', assume_sorted=True)\n",
    "        G_Raw[i][g_bad] = g_interp(Time[g_bad])\n",
    "    \n",
    "    r_bad = np.isnan(R_Raw[i])\n",
    "    if r_bad.any():\n",
    "        r_interp = interp.interp1d(Time[~r_bad], R_Raw[i,~r_bad], kind='cubic', assume_sorted=True)\n",
    "        R_Raw[i][r_bad] = r_interp(Time[r_bad])\n",
    "    \n",
    "    #print(i)\n",
    "    \n",
    "    ### Visualize interpolated points\n",
    "#     plt.scatter(Time[~r_bad], R_Raw[i,~r_bad], color='blue')\n",
    "#     plt.plot(Time, R_Raw[i], color='blue', alpha=0.2)\n",
    "#     plt.scatter(Time[r_bad], R_Raw[i,r_bad], color='red')\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,10))\n",
    "for neuron in range(neuron_length):\n",
    "    plt.plot(R_Raw[neuron,:])\n",
    "plt.title(\"GFP RFP worm RFP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = mean(R_Raw, axis=1)\n",
    "v = np.var(R_Raw, axis=1)\n",
    "print(\"mean: %1.5f, variance: %1.5f\" % (np.mean(m), np.mean(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressing against all RFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define ridge regression, maximizing marginal likelihood (see Bishop p.167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define function to do ridge regression for an individual neuron\n",
    "def ridge_regress(X, Y, \n",
    "                  XTX = None, eigs = None, \n",
    "                  converge_required = 0.001, a = 1.0, b = 1.0,\n",
    "                  a_max = 100000000,\n",
    "                  printer = False, verbose = False):\n",
    "    \n",
    "    ### Initialize\n",
    "    a_old = -1.0\n",
    "    N = len(X)\n",
    "\n",
    "    if XTX == None:\n",
    "        XTX = np.dot(X.T,X)\n",
    "\n",
    "    if eigs == None:\n",
    "        eigs = np.linalg.eigvals(XTX)\n",
    "\n",
    "    ### Loop until alpha converges\n",
    "    iterations = 0\n",
    "    while abs(a_old - a) > converge_required and a < a_max:\n",
    "        \n",
    "        if np.isinf(a) and verbose:\n",
    "            print(\"These features are not helpful in prediction for ridge regression\")\n",
    "\n",
    "        # Sigma = (b*XTX + A)^-1\n",
    "        Sigma = np.linalg.inv(b*XTX + a*np.eye(len(XTX)))\n",
    "\n",
    "        # mu = b*Sigma*X.T*Y\n",
    "        mu = b*np.dot(np.dot(Sigma, X.T), Y)\n",
    "\n",
    "        gamma = np.sum([(b*i)/(a + b*i) for i in eigs])\n",
    "        a_new = gamma/np.dot(mu.T,mu)\n",
    "\n",
    "        error = np.sum((Y - np.dot(X, mu))**2)\n",
    "        b_new = (N - gamma)/error\n",
    "\n",
    "        # debugging\n",
    "        if verbose:\n",
    "            if np.isinf(a):\n",
    "                print(\"\\tOld: %1.5f, New: %1.5f\" %(a_old, a))\n",
    "        \n",
    "        \n",
    "        a_old = a\n",
    "        a = a_new\n",
    "        b = b_new\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "        if printer:\n",
    "            print(iterations, \"    alpha = \", a, \" beta = \", b, \" Squared-Error = \", error) \n",
    "    converged = abs(a_old-a)< converge_required\n",
    "    if verbose:\n",
    "        if converged: \n",
    "            print(\"\\tIterations: %d\" % (iterations))\n",
    "        else: \n",
    "            print(\"\\tIterations: %d, a_old: %1.5f , Converged within: %1.5f\" % (iterations, a_old, abs(a_old-a)))\n",
    "    conf_int = np.sqrt(np.diag(Sigma)) # See Bishop p.167\n",
    "    \n",
    "    ### Return regression weights 'mu', std of weights 'conf_int', squared error of regression 'error'\n",
    "    ## Converged: boolean whether or not the iterations converged to convergence required. \n",
    "    return mu, conf_int, error, converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define (Group) ARD for selecting only relevant weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GroupARD(X, Y, in_group = 1, \n",
    "             alpha_max = 100000000, max_iterations = 1000, printer=False):\n",
    "\n",
    "    N = len(X)\n",
    "    M = len(X[0])\n",
    "    \n",
    "    a = np.repeat(1.0, M) # alphas\n",
    "    b = 1.0               # Beta = 1/sig^2 \n",
    "\n",
    "    X1 = X\n",
    "\n",
    "    deletions = []\n",
    "    old_alphas = [a]\n",
    "\n",
    "    for ard_iteration in range(max_iterations):\n",
    "\n",
    "        # Sigma = (b*XTX + A)^-1\n",
    "        Sigma = np.linalg.inv(b*np.dot(X1.T,X1) + np.diag(a))\n",
    "\n",
    "        # mu = b*Sigma*X.T*Y\n",
    "        mu = b*np.dot(np.dot(Sigma, X1.T), Y)\n",
    "\n",
    "        gamma = 1.0 - a*np.diag(Sigma)\n",
    "        group_gamma = np.array([np.sum(gamma[i:i+in_group]) for i in range(0, len(gamma), in_group)])\n",
    "        mu_squared = mu**2\n",
    "        group_mu = np.array([np.sum(mu_squared[i:i+in_group]) for i in range(0, len(mu_squared), in_group)])\n",
    "        a_new = group_gamma/group_mu\n",
    "\n",
    "        error = np.sum((Y - np.dot(X1, mu))**2)\n",
    "        b_new = (N - np.sum(gamma))/error\n",
    "\n",
    "        a = [alpha for alpha in a_new for k in range(in_group)]\n",
    "        b = b_new\n",
    "\n",
    "        if printer : print(\"\\nIteration: \", ard_iteration, \" beta = \", b, \" Squared-Error = \", error)  \n",
    "\n",
    "        over = [i for i in range(len(a)) if a[i] > alpha_max]\n",
    "        if over:\n",
    "            if printer : print(\"Deletions: \", len(over))\n",
    "            deletions = [over] + deletions\n",
    "            X1 = np.delete(X1,over,axis=1)\n",
    "            a = np.delete(a,over)\n",
    "        else:\n",
    "            a_converge = np.sum((a - np.array(old_alphas[-1]))**2)\n",
    "            if printer : print(\"Alpha distance = \", a_converge, \"   Max alpha = \", np.max(a))\n",
    "            if a_converge < .00001:\n",
    "                break\n",
    "        \n",
    "        old_alphas.append(a)\n",
    "\n",
    "\n",
    "    # Recover mu\n",
    "    for i in deletions:\n",
    "        for j in i:\n",
    "            a = np.insert(a,j,-1)\n",
    "            mu = np.insert(mu,j,0)\n",
    "\n",
    "    if printer: \n",
    "        df = pd.DataFrame(list(zip(a, mu)), columns = ['alpha', 'mu'])\n",
    "        print(\"\\n\", df)\n",
    "        print(\"\\nDeletions:\", np.sum([len(i) for i in deletions])/in_group, 'out of', M/in_group)\n",
    "        \n",
    "    return mu, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate MSE for different regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MSE(G_Raw, Time, R_Raw, \n",
    "        use_R_Raw = True, num_timelag=0, num_timejump=0,\n",
    "        CL_PCs = None, num_CL_comp = 1, CL_timelag = 0, CL_timejump = 0, \n",
    "        NP_PCs = None, num_NP_comp = 1, NP_timelag = 0, NP_timejump = 0,\n",
    "        percent_holdout=20, reg_type='all', alpha_max = 100000,\n",
    "        verbose = False):\n",
    "\n",
    "    [neuron_length, neuron_time] = np.shape(G_Raw)\n",
    "\n",
    "    ### Initialize design matrix M ###\n",
    "    if use_R_Raw:\n",
    "        primary_window = num_timelag + num_timejump + 1\n",
    "    else:\n",
    "        primary_window = 0; num_timelag = 0; num_timejump = 0\n",
    "    \n",
    "    if CL_PCs is not None: \n",
    "        CL_window = CL_timelag + CL_timejump + 1\n",
    "    else:\n",
    "        num_CL_comp = 0\n",
    "        CL_window = 0; CL_timelag = 0; CL_timejump = 0\n",
    "    \n",
    "    if NP_PCs is not None: \n",
    "        NP_window = NP_timelag + NP_timejump + 1\n",
    "    else:\n",
    "        num_NP_comp = 0\n",
    "        NP_window = 0; NP_timelag = 0; NP_timejump = 0      \n",
    "        \n",
    "    max_window = np.max([primary_window, CL_window, NP_window])\n",
    "    max_timelag = np.max([num_timelag, CL_timelag, NP_timelag])\n",
    "    max_timejump = np.max([num_timejump, CL_timejump, NP_timejump])\n",
    "\n",
    "    full_window = primary_window*neuron_length + CL_window*num_CL_comp + NP_window*num_NP_comp\n",
    "        \n",
    "    # Matrix of times x neural values in window, pc projections\n",
    "    M = np.zeros((neuron_time - max_window + 1, full_window))\n",
    "    for i in np.arange(max_timelag, neuron_time - max_timejump):\n",
    "        \n",
    "        b = 0 \n",
    "        if use_R_Raw:\n",
    "            for j in range(neuron_length):\n",
    "                M[i - max_timelag][b + j*primary_window:b + (j+1)*primary_window] = R_Raw[j,i-num_timelag:i+num_timejump+1]\n",
    "        \n",
    "        b = b + neuron_length*primary_window\n",
    "        for k in range(num_CL_comp):\n",
    "            M[i - max_timelag][b + k*CL_window:b + (k+1)*CL_window] = CL_PCs[k,i-CL_timelag:i+CL_timejump+1]\n",
    "            \n",
    "        b = b + num_CL_comp*CL_window\n",
    "        for z in range(num_NP_comp):\n",
    "            M[i - max_timelag][b + z*NP_window:b + (z+1)*NP_window] = NP_PCs[z,i-NP_timelag:i+NP_timejump+1]\n",
    "\n",
    "    \n",
    "    Y_all = G_Raw.T[max_timelag:neuron_time-max_timejump]\n",
    "    Time_all = Time[max_timelag:neuron_time-max_timejump]\n",
    "\n",
    "    #####################################################\n",
    "\n",
    "    ### Split data into TRAIN and TEST set ###\n",
    "    \n",
    "    ### Define the size of the holdout set\n",
    "    time_index_split = int(len(Y_all)*(1 - percent_holdout/100.0))\n",
    "\n",
    "    ### Define TRAINing data\n",
    "    TRAIN_Y_all = Y_all[:time_index_split]\n",
    "    TRAIN_M = M[:time_index_split]\n",
    "    TRAIN_Time = Time_all[:time_index_split]\n",
    "    \n",
    "    ### Remove mean from Y\n",
    "    mean_TRAIN_Y = np.mean(TRAIN_Y_all, axis=0)\n",
    "    TRAIN_Y_all = TRAIN_Y_all - mean_TRAIN_Y\n",
    "\n",
    "    ### Define TESTing data\n",
    "    TEST_Y_all = Y_all[time_index_split:] - mean_TRAIN_Y\n",
    "    TEST_M = M[time_index_split:]\n",
    "    TEST_Time = Time_all[time_index_split:]\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    ### Run regressions on TRAIN data ###\n",
    "    \n",
    "    ARD_mu = None; ARD_alpha = None; ridge_mu = None; standard_mu = None\n",
    "    \n",
    "    ### ARD\n",
    "    if reg_type in ['ard', 'all']:\n",
    "        \n",
    "        ### Run GroupARD() on all neruons to get all weights\n",
    "        ARD_mu = np.zeros((neuron_length, full_window))\n",
    "        ARD_alpha = np.zeros((neuron_length, full_window))\n",
    "\n",
    "        for i in range(len(ARD_mu)):\n",
    "            ARD_mu[i], ARD_alpha[i] = GroupARD(TRAIN_M, TRAIN_Y_all[:,i], in_group=1, alpha_max = alpha_max)\n",
    "            \n",
    "    ### Ridge Regression\n",
    "    if reg_type in ['ridge', 'ridge_std', 'all']:\n",
    "        \n",
    "        ### Run ridge_regression() on all neruons to get all weights\n",
    "        ridge_mu = np.zeros((neuron_length, full_window))\n",
    "        XTX = TRAIN_M.T @ TRAIN_M\n",
    "        eigs = np.linalg.eigvals(XTX)\n",
    "\n",
    "        number_converged = 0\n",
    "        for i in range(len(ridge_mu)):\n",
    "            if verbose:\n",
    "                print(\"Neuron %d\" % i)\n",
    "            ridge = ridge_regress(TRAIN_M, TRAIN_Y_all[:,i], XTX = XTX, eigs = eigs, verbose = verbose)\n",
    "            ridge_mu[i] = ridge[0]\n",
    "            if ridge[3] == True:\n",
    "                number_converged = number_converged + 1\n",
    "        # debugging\n",
    "        print('Number of neurons that converged: %d, total neurons: %d, percentage: %2.1f' %\n",
    "              (number_converged, len(ridge_mu), number_converged*100/len(ridge_mu)))\n",
    "      \n",
    "    ### Standard Regression\n",
    "    if reg_type in ['standard', 'ridge_std', 'all']:\n",
    "        standard_mu = np.linalg.solve(TRAIN_M.T @ TRAIN_M , TRAIN_M.T @ TRAIN_Y_all).T\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    ### Compute MSE on TEST data ###\n",
    "    ridge_MSE = None; standard_MSE = None; ARD_MSE = None\n",
    "    \n",
    "    ### Standard regression on single neuron\n",
    "    single_MSE = np.zeros((neuron_length))\n",
    "    for i in range(neuron_length):\n",
    "        slope, intercept = stats.linregress(R_Raw[i,:time_index_split],G_Raw[i,:time_index_split])[:2]\n",
    "        single_MSE[i] = np.average((slope*R_Raw[i,time_index_split:] + intercept - G_Raw[i,time_index_split:])**2)\n",
    "      \n",
    "    ### Ridge regression on all neurons\n",
    "    if reg_type in ['ridge', 'ridge_std', 'all']:\n",
    "        ridge_MSE = np.zeros((neuron_length))\n",
    "        for i in range(neuron_length):\n",
    "            ridge_MSE[i] = np.average(((TEST_M @ ridge_mu[i]) - TEST_Y_all.T[i])**2)\n",
    "    \n",
    "    ### Standard regression on all neurons\n",
    "    if reg_type in ['standard', 'ridge_std', 'all']:\n",
    "        standard_MSE = np.zeros((neuron_length))\n",
    "        for i in range(neuron_length):\n",
    "            standard_MSE[i] = np.average(((TEST_M @ standard_mu[i])- TEST_Y_all.T[i])**2)\n",
    "\n",
    "    ### Group ARD on all neurons\n",
    "    if reg_type in ['ard', 'all']:\n",
    "        ARD_MSE = np.zeros((neuron_length))\n",
    "        for i in range(neuron_length):\n",
    "            ARD_MSE[i] = np.average(((TEST_M @ ARD_mu[i]) - TEST_Y_all.T[i])**2)\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    return {'ridge_mu' : ridge_mu, 'standard_mu' : standard_mu, 'ARD_mu' : ARD_mu,\n",
    "            'single_MSE' : single_MSE, 'ridge_MSE' : ridge_MSE, 'standard_MSE' : standard_MSE, 'ARD_MSE' : ARD_MSE,\n",
    "            'M' : M, 'Y_all' : Y_all, 'Time_all' : Time_all, \n",
    "            'time_index_split' : time_index_split, 'mean_TRAIN_Y' : mean_TRAIN_Y,\n",
    "            'ARD_alpha' : ARD_alpha}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Unpack output from MSE()\n",
    "out = MSE(G_Raw,Time,R_Raw,\n",
    "          #use_R_Raw = False,\n",
    "          #NP_PCs = NP_PCs, num_NP_comp=1,#NP_timelag=10, NP_timejump=10,\n",
    "          #CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "          reg_type='ridge')\n",
    "ridge_mu, standard_mu, single_MSE, ridge_MSE, standard_MSE, ARD_MSE, M, Y_all, Time_all, time_index_split, mean_TRAIN_Y, ARD_mu, ARD_alpha = out['ridge_mu'], out['standard_mu'], out['single_MSE'], out['ridge_MSE'], out['standard_MSE'], out['ARD_MSE'], out['M'], out['Y_all'], out['Time_all'], out['time_index_split'], out['mean_TRAIN_Y'], out['ARD_mu'], out['ARD_alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 29\n",
    "plt.plot(ridge_mu[i])\n",
    "print(ridge_MSE[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Read in worm data from SharedData .npz file\n",
    "Worm = 'GFP'\n",
    "worm_data = np.load('../SharedData/Worm'+Worm+'.npz')\n",
    "print('The loaded npz contains the variables:\\n', np.sort([i for i in worm_data]))\n",
    "\n",
    "G_sig = 'GFP' if Worm == 'GFP' else 'GCaMP'\n",
    "\n",
    "### Import desired variables\n",
    "NPos = worm_data['NPos']\n",
    "Time = worm_data['Time']\n",
    "\n",
    "NPos = np.transpose(NPos, (1,2,0)) # Reorder: neuron, dimension, time\n",
    "Time = Time[:,0]\n",
    "\n",
    "original_neurons,original_dim, original_time = NPos.shape\n",
    "print('Shape of Neuron Position Matrix:', NPos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "### Fill in NaNs with interpolation\n",
    "all_bad = np.zeros((len(Time))).astype(bool)\n",
    "for i in range(len(NPos)):\n",
    "    for j in range(3):\n",
    "        \n",
    "        bad = np.isnan(NPos[i][j]) | (NPos[i][j]==1.0)\n",
    "        all_bad = all_bad | bad\n",
    "\n",
    "        interp_pos = interp1d(Time[~bad], NPos[i][j][~bad], kind='linear', \n",
    "                              assume_sorted=True, bounds_error=False)\n",
    "        NPos[i][j][bad] = interp_pos(Time[bad])\n",
    "\n",
    "        ### Visualize interpolated points\n",
    "#         plt.scatter(Time[~bad], NPos[i][j][~bad], color='blue')\n",
    "#         plt.plot(Time, NPos[i][j], color='blue', alpha=0.2)\n",
    "#         plt.scatter(Time[bad], NPos[i][j][bad], color='red')\n",
    "#         plt.show()\n",
    "#         break\n",
    "\n",
    "### Define the first and last 'good' point (able to be interpolated), and trim accordingly\n",
    "begin = np.where(~all_bad)[0][0]\n",
    "end = np.where(~all_bad)[0][-1] + 1\n",
    "NPos = NPos[:,:,begin:end]\n",
    "Time = Time[begin:end]\n",
    "\n",
    "### Transform z-coordinate from volts in pixels\n",
    "volt_to_pixel = 30\n",
    "NPos[:,2,:] = NPos[:,2,:]*volt_to_pixel\n",
    "\n",
    "### Get sizes\n",
    "num_neuron, dim, num_time = NPos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NPos = np.transpose(NPos, (2,0,1)) # Reorder: time, neuron, dimension\n",
    "dists_far = np.zeros((num_time, num_neuron, num_neuron)) # initialize matrix for pairwise distances\n",
    "### Fill in pairwise distance matrix across time\n",
    "for i in range(num_time):\n",
    "    for j in range(len(NPos[i])): # number neurons\n",
    "        for k in np.arange(j):\n",
    "            dists_far[i][j][k] = np.linalg.norm(NPos[i][j] - NPos[i][k]) # function returns euclidean distance\n",
    "            dists_far[i][k][j] = np.linalg.norm(NPos[i][j] - NPos[i][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dists = np.mean(dists_far, axis = 0)\n",
    "dists_var = np.var(dists_far, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# i = 39\n",
    "#plt.scatter(dists[i]/np.max(dists[i]), abs(ridge_mu[i])/ridge_mu[i,i])\n",
    "# plt.scatter(dists[i], ridge_mu[i], color='blue')\n",
    "# plt.scatter(dists[i,i], ridge_mu[i,i,], color='red')\n",
    "# plt.ylabel('Regression Weight of Neruon')\n",
    "# plt.xlabel('Average Pairwise Distance')\n",
    "# plt.figtext(0.7,0.8, \"pMSE = \" + str(int(ridge_MSE[i])), fontsize=14)\n",
    "# plt.figtext(0.7,0.75, \"n = \" + str(i), fontsize=14)\n",
    "# plt.scatter(ridge_mu[i], dists_var[i])\n",
    "# plt.ylim([-5,250])\n",
    "\n",
    "neuron_n = [3,8,10,28]  # List of neurons to be observed\n",
    "\n",
    "f ,ax = plt.subplots(len(neuron_n),1, figsize=(8,4*len(neuron_n)))\n",
    "for i,j in enumerate(neuron_n):\n",
    "    ax[i].scatter(dists[j], ridge_mu[j], color='blue')\n",
    "    ax[i].scatter(dists[j,j], ridge_mu[j,j], color='red')\n",
    "    \n",
    "    ax[i].text(0.7,0.8, \"pMSE = \" + str(int(ridge_MSE[j])), fontsize=14, transform=ax[i].transAxes)\n",
    "    ax[i].text(0.7,0.73, \"n = \" + str(j), fontsize=14, transform=ax[i].transAxes)\n",
    "    ax[i].set_xlabel('Average Pairwise Distance')\n",
    "    ax[i].set_ylabel('Regression Weight of Neruon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "errors = []\n",
    "for i in np.arange(10):\n",
    "    print(i)\n",
    "#     temp = MSE(R_Raw,G_Raw,Time, NP_PCs = NP_PCs, num_NP_comp=i, NP_timelag=5, NP_timejump=5)\n",
    "#     temp = MSE(R_Raw,G_Raw,Time, CL_PCs = CL_PCs, num_CL_comp=6, CL_timelag=i, CL_timejump=i)\n",
    "    temp = MSE(R_Raw,G_Raw,Time, num_timelag=i, num_timejump=i)\n",
    "    results.append(temp)\n",
    "    errors.append(temp['ridge_MSE']/temp['single_MSE'])\n",
    "    \n",
    "print(pd.DataFrame(np.array(errors).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(errors[0], bins=60, range = (0,1.5), color = 'red', alpha=0.5, cumulative=True)\n",
    "plt.hist(errors[1], bins=60, range = (0,1.5), color = 'green', alpha=0.5, cumulative=True)\n",
    "plt.hist(errors[2], bins=60, range = (0,1.5), color = 'blue', alpha=0.5, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(errors)):\n",
    "    print(i, np.mean(errors[i]), np.median(errors[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(errors)):\n",
    "    print(i, np.mean(errors[i]), np.median(errors[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(errors)):\n",
    "    print(i, np.mean(errors[i]), np.median(errors[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(standard_mu[0], color = 'blue')\n",
    "plt.plot(ridge_mu[0], color = 'green')\n",
    "# plt.plot(ARD_mu[0], color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[2,2],[4,4]])\n",
    "np.average(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing if Neuron Position and Centerline make a difference to MSE\n",
    "\n",
    "Setting close to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Unpack output from MSE()\n",
    "outNone = MSE(G_Raw,Time,R_Raw,\n",
    "          reg_type='ridge_std')\n",
    "\n",
    "### Unpack output from MSE()\n",
    "outNP = MSE(G_Raw,Time,R_Raw,\n",
    "          NP_PCs = NP_PCs, num_NP_comp=1, #NP_timelag=10, NP_timejump=10,\n",
    "          reg_type='ridge_std')\n",
    "\n",
    "outNPCL = MSE(G_Raw,Time,R_Raw,\n",
    "          NP_PCs = NP_PCs, num_NP_comp=1, #NP_timelag=10, NP_timejump=10,\n",
    "          CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "          reg_type='ridge_std')\n",
    "\n",
    "outCL = MSE(G_Raw,Time,R_Raw,\n",
    "          CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "          reg_type='ridge_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].plot(outNone['ridge_MSE'].T)\n",
    "ax[0].plot(outNP['ridge_MSE'].T)\n",
    "ax[0].plot(outNPCL['ridge_MSE'].T)\n",
    "ax[0].plot(outCL['ridge_MSE'].T)\n",
    "ax[1].plot(outNone['standard_MSE'].T)\n",
    "ax[1].plot(outNP['standard_MSE'].T)\n",
    "ax[1].plot(outNPCL['standard_MSE'].T)\n",
    "ax[1].plot(outCL['standard_MSE'].T)\n",
    "\n",
    "if Close:\n",
    "    ax[0].set_title(\"Ridge MSE (no tail, close pairwise)\")\n",
    "    ax[1].set_title(\"Standard (no tail, close pairwise)\")\n",
    "else:\n",
    "    ax[0].set_title(\"Ridge MSE (with tail, pairwise)\")\n",
    "    ax[1].set_title(\"Standard (with tail, pairwise)\")\n",
    "ax[0].legend(['None', 'Neuron positons', 'Neuron pos & centerline', 'Centerline'])\n",
    "ax[1].legend(['None', 'Neuron positons', 'Neuron pos & centerline', 'Centerline'])\n",
    "ax[0].set_xlabel('Neuron')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[1].set_xlabel('Neuron')\n",
    "ax[1].set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_MSE = np.zeros((10, neuron_length))\n",
    "standard_MSE = np.empty_like(ridge_MSE)\n",
    "for i in np.arange(10):\n",
    "    out = MSE(G_Raw,Time,R_Raw,\n",
    "                  NP_PCs = NP_PCs, num_NP_comp=1, NP_timelag=i, NP_timejump=i,\n",
    "                  CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "                  reg_type='ridge_std')\n",
    "    ridge_MSE[i,:] = out['ridge_MSE']\n",
    "    standard_MSE[i,:] = out['standard_MSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "legend_names = []\n",
    "for i in np.arange(10):\n",
    "    legend_names.append(\"%d timelag\"%i)\n",
    "    ax[0].plot(ridge_MSE[i,:])\n",
    "    ax[1].plot(standard_MSE[i,:])\n",
    "    \n",
    "if Close:\n",
    "    ax[0].set_title(\"Ridge MSE exp pairwise distances\\nno tail with centerlines\")\n",
    "    ax[1].set_title(\"Standard MSE exp pairwise distances\\nno tail with centerlines\")\n",
    "else:\n",
    "    ax[0].set_title(\"Ridge MSE pairwise distances\\nwith tail with centerlines\")\n",
    "    ax[1].set_title(\"Standard MSE pairwise distances\\nwith tail with centerlines\")\n",
    "ax[0].legend(legend_names)\n",
    "ax[1].legend(legend_names)\n",
    "ax[0].set_xlabel('Neuron')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[1].set_xlabel('Neuron')\n",
    "ax[1].set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress w/o RFP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outNone = MSE(G_Raw,Time, R_Raw,\n",
    "          #NP_PCs = NP_PCs, num_NP_comp=1, #NP_timelag=10, NP_timejump=10,\n",
    "          #CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "          reg_type='ridge_std', use_R_Raw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outNP = MSE(G_Raw,Time, R_Raw, \n",
    "          NP_PCs = NP_PCs, num_NP_comp=1, #NP_timelag=10, NP_timejump=10,\n",
    "          #CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "          reg_type='ridge_std', use_R_Raw = False)\n",
    "\n",
    "outNPCL = MSE(G_Raw,Time, R_Raw, \n",
    "          NP_PCs = NP_PCs, num_NP_comp=1, #NP_timelag=10, NP_timejump=10,\n",
    "          CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "          reg_type='ridge_std', use_R_Raw = False)\n",
    "\n",
    "outCL = MSE(G_Raw,Time, R_Raw, \n",
    "          #NP_PCs = NP_PCs, num_NP_comp=1, #NP_timelag=10, NP_timejump=10,\n",
    "          CL_PCs = CL_PCs, num_CL_comp=1,\n",
    "          reg_type='ridge_std', use_R_Raw = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].plot(outNone['ridge_MSE'].T)\n",
    "ax[0].plot(outNP['ridge_MSE'].T)\n",
    "ax[0].plot(outNPCL['ridge_MSE'].T)\n",
    "ax[0].plot(outCL['ridge_MSE'].T)\n",
    "ax[1].plot(outNone['standard_MSE'].T)\n",
    "ax[1].plot(outNP['standard_MSE'].T)\n",
    "ax[1].plot(outNPCL['standard_MSE'].T)\n",
    "ax[1].plot(outCL['standard_MSE'].T)\n",
    "\n",
    "if Close:\n",
    "    ax[0].set_title(\"Ridge MSE No RFP (no tail, close pairwise)\")\n",
    "    ax[1].set_title(\"Standard (no tail, close pairwise)\")\n",
    "else:\n",
    "    ax[0].set_title(\"Ridge MSE No RFP (with tail, pairwise)\")\n",
    "    ax[1].set_title(\"Standard (with tail, pairwise)\")\n",
    "ax[0].legend(['RFP', 'Neuron positons', 'Neuron pos & centerline', 'Centerline'])\n",
    "ax[1].legend(['RFP', 'Neuron positons', 'Neuron pos & centerline', 'Centerline'])\n",
    "ax[0].set_xlabel('Neuron')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[1].set_xlabel('Neuron')\n",
    "ax[1].set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Look at distribution of NaNs in original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Get list of columns with any NaN present\n",
    "badcols = np.array([x|y for (x,y) in zip(np.isnan(G_Raw).any(axis=0), np.isnan(R_Raw).any(axis=0))])\n",
    "\n",
    "### NaN Distribution Information\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(len(np.nonzero(badcols)[0]), 'columns/times containing NaNs were found')\n",
    "\n",
    "print('\\nNaN Count by Time Column, Out of', len(G_Raw), 'Neurons')\n",
    "print(pd.DataFrame(np.column_stack((Time[badcols],np.sum(np.isnan(G_Raw[:,badcols]), axis=0),np.sum(np.isnan(R_Raw[:,badcols]), axis=0))), \n",
    "                   index = np.nonzero(badcols)[0], columns=['Time', 'G_Raw', 'R_Raw']))\n",
    "\n",
    "print('\\nNaN Count by Neruon, Out of', len(G_Raw[0]), 'Time Points')\n",
    "print(pd.DataFrame(np.column_stack((np.sum(np.isnan(G_Raw), axis=1),np.sum(np.isnan(R_Raw), axis=1))), \n",
    "                   columns=['G_Raw', 'R_Raw']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove all columns containing any NaN, instead of interpolating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Remove Columns containing NaNs\n",
    "badcols = np.array([x|y for (x,y) in zip(np.isnan(G_Raw).any(axis=0), np.isnan(R_Raw).any(axis=0))])\n",
    "\n",
    "G_Raw = G_Raw[:,~badcols]\n",
    "R_Raw = R_Raw[:,~badcols]\n",
    "Time = Time[~badcols]\n",
    "\n",
    "[neuron_length, neuron_time] = np.shape(G_Raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a heat map of GFP/GCaMP & RFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Heat Map of GFP/GCaMP & RFP across trial\n",
    "f ,ax = plt.subplots(1,2, figsize=(16,10*2))\n",
    "ax[0].imshow(G_Raw, aspect=10,cmap=\"Paired\")\n",
    "ax[1].imshow(R_Raw, aspect=10,cmap=\"Paired\")\n",
    "for axis, title in zip(ax, [G_sig, r'RFP']):\n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel(r'Frame #')\n",
    "    axis.set_ylabel(r'Neuron #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examine Raw Fluorescence statistics within worm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize distribution of Raw fluorescence statistics across neurons\n",
    "p = 8 # fluorescence percentile\n",
    "\n",
    "f, ax = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "ax[0][0].hist(np.min(G_Raw, axis=1), color='green')\n",
    "ax[0][1].hist(np.percentile(G_Raw, p, axis=1), color='green')\n",
    "ax[0][2].hist(np.median(G_Raw, axis=1), color='green')\n",
    "ax[0][3].hist(np.max(G_Raw, axis=1), color='green')\n",
    "\n",
    "ax[1][0].hist(np.min(R_Raw, axis=1), color='red')\n",
    "ax[1][1].hist(np.percentile(R_Raw, p, axis=1), color='red')\n",
    "ax[1][2].hist(np.median(R_Raw, axis=1), color='red')\n",
    "ax[1][3].hist(np.max(R_Raw, axis=1), color='red')\n",
    "\n",
    "ax[0][0].set_title(G_sig + ', Minimum Raw', fontsize = 16)\n",
    "ax[0][1].set_title(G_sig + ', ' + str(p) + 'th Percentile Raw', fontsize = 16)\n",
    "ax[0][2].set_title(G_sig + ', Median Raw', fontsize = 16)\n",
    "ax[0][3].set_title(G_sig + ', Maximum Raw', fontsize = 16)\n",
    "ax[1][0].set_title('RFP, Minimum Raw', fontsize = 16)\n",
    "ax[1][1].set_title('RFP, ' + str(p) + 'th Percentile Raw', fontsize = 16)\n",
    "ax[1][2].set_title('RFP, Median Raw', fontsize = 16)\n",
    "ax[1][3].set_title('RFP, Maximum Raw', fontsize = 16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Data/Worm'+Worm+'Raw_Fluorescence_Dist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize raw activity (optionally rescaled) of particular neruons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize Raw Activity of specific neruon (optionally rescaled to 0-1)\n",
    "def rescale(a):\n",
    "    return [(i-np.min(a))/(np.max(a)-np.min(a)) for i in a]\n",
    "\n",
    "n = 12\n",
    "f = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(Time, rescale(G_Raw[n]), color = 'green')\n",
    "plt.plot(Time, rescale(R_Raw[n]), color = 'red')\n",
    "\n",
    "# plt.plot(Time, G_Raw[n], color = 'green')\n",
    "# plt.plot(Time, R_Raw[n], color = 'red')\n",
    "\n",
    "plt.xlim([0,Time[-1]])\n",
    "\n",
    "plt.show()\n",
    "print('Neuron: ', n)\n",
    "print('            Min       %  Median     Max')\n",
    "print('G_Raw: ', '%7.1f' % np.min(G_Raw[n]), '%7.1f' % np.percentile(G_Raw[n],8), '%7.1f' % np.median(G_Raw[n]), '%7.1f' % np.max(G_Raw[n]))\n",
    "print('R_Raw: ', '%7.1f' % np.min(R_Raw[n]), '%7.1f' % np.percentile(R_Raw[n],8), '%7.1f' % np.median(R_Raw[n]), '%7.1f' % np.max(R_Raw[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do basic linear regression of GFP/GCaMP against RFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Caclualte regression info of GFP/GCaMP against RFP\n",
    "slopes = np.zeros(neuron_length)\n",
    "r_values = np.zeros(neuron_length)\n",
    "intercepts = np.zeros(neuron_length)\n",
    "\n",
    "for i in range(neuron_length):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(R_Raw[i],G_Raw[i])\n",
    "    slopes[i] = slope\n",
    "    r_values[i] = r_value\n",
    "    intercepts[i] = intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Coefficeint of Varaiation figure for RFP vs GFP w/ r^2 as colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Display Coefficeint of Variation for RFP vs GFP w/ r^2 as colorbar\n",
    "def onpick3(event):\n",
    "    ind = event.ind\n",
    "    print('onpick3 scatter:', ind)\n",
    "\n",
    "f = plt.figure(figsize=(10,10))\n",
    "plt.scatter(stats.variation(R_Raw, axis=1),stats.variation(G_Raw, axis=1), c = r_values, cmap='hot', picker=True)\n",
    "plt.scatter(np.var(R_Raw, axis=1),np.var(G_Raw, axis=1), c = r_values, cmap='hot', picker=True)\n",
    "plt.plot(np.arange(0.01,0.6,.01),np.arange(0.01,0.6,.01))\n",
    "plt.xlim([0,.6])\n",
    "plt.ylim([0,.6])\n",
    "plt.xlabel('Cof. of Var. RFP Raw')\n",
    "plt.ylabel('Cof. of Var. GFP Raw')\n",
    "plt.colorbar(label=r'$r^2$')\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(stats.variation(R_Raw, axis=1),stats.variation(G_Raw, axis=1))\n",
    "print(slope, intercept, r_value**2, p_value, std_err)\n",
    "plt.plot(np.arange(0.01,0.6,.01), intercept + slope*np.arange(0.01,0.6,.01), 'g--')\n",
    "\n",
    "f.canvas.mpl_connect('pick_event', onpick3)\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('Coeffcient_of_Variation.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize fit of weights to neural activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define which weights to use & neruons to see\n",
    "weights = ridge_mu\n",
    "neuron_n = [52,55,12,42]  # List of neurons to be observed\n",
    "\n",
    "f ,ax = plt.subplots(len(neuron_n),2, figsize=(16,4*len(neuron_n)))\n",
    "for i,j in enumerate(neuron_n):\n",
    "#     ax[i][0].plot(Time_all, Y_all.T[j]) # Actual activity\n",
    "#     ax[i][0].plot(Time_all, (M @ weights[j])) # Recovered Activity using Estimated Weights\n",
    "#     ax[i][1].plot(Time_all, (M @ weights[j]) - Y_all.T[j]) # Error\n",
    "    \n",
    "    ax[i][0].plot(Time_all[time_index_split:], Y_all.T[j,time_index_split:]) # Actual activity\n",
    "    ax[i][0].plot(Time_all[time_index_split:], (M @ weights[j] + mean_TRAIN_Y[j])[time_index_split:]) # Recovered Activity using Estimated Weights\n",
    "    #ax[i][0].plot(Time_all[time_index_split:], (M @ ARD_mu[j] + mean_TRAIN_Y[j])[time_index_split:]) # Recovered Activity using Estimated Weights\n",
    "    ax[i][1].plot(Time_all[time_index_split:], ((M @ weights[j]) - Y_all.T[j] + mean_TRAIN_Y[j])[time_index_split:]) # Error\n",
    "    \n",
    "    ax[i][0].set_ylabel(r'Neuron ' + str(neuron_n[i]))\n",
    "    \n",
    "for axis, title in zip(ax[0], [r'Actual Activity & Estimated Activity', r'Error']):\n",
    "    axis.set_title(title)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define which weights to use & neruons to see\n",
    "weights = ridge_mu\n",
    "neuron_n = [12,42]  # List of neurons to be observed\n",
    "\n",
    "f ,ax = plt.subplots(len(neuron_n),1, figsize=(12,4*len(neuron_n)))\n",
    "for i,j in enumerate(neuron_n):\n",
    "#     ax[i][0].plot(Time_all, Y_all.T[j]) # Actual activity\n",
    "#     ax[i][0].plot(Time_all, (M @ weights[j])) # Recovered Activity using Estimated Weights\n",
    "#     ax[i][1].plot(Time_all, (M @ weights[j]) - Y_all.T[j]) # Error\n",
    "    \n",
    "    ax[i].plot(Time_all[time_index_split:], Y_all.T[j,time_index_split:], label='Actual Activity') # Actual activity\n",
    "    ax[i].plot(Time_all[time_index_split:], (M @ weights[j] + mean_TRAIN_Y[j])[time_index_split:], label='Predicted Activity') # Recovered Activity using Estimated Weights\n",
    " \n",
    "    ax[i].set_title(r'Neuron #' + str(neuron_n[i]))\n",
    "    ax[i].set_xlabel(r'Time (s)')\n",
    "    ax[i].set_ylabel(r'Raw GFP Fluorescence')\n",
    "    ax[i].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a[1:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.maximum(0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
