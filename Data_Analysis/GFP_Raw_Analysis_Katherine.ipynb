{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import animation, pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.linalg as linalg\n",
    "import scipy.stats as stats\n",
    "import scipy.interpolate as interp\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Read in worm data from SharedData .npz file\n",
    "Worm = 'GFP'\n",
    "worm_data = np.load('../SharedData/Worm_Angles/WormAngle'+Worm+'.npz')\n",
    "print('The loaded npz contains the variables:\\n', np.sort([i for i in worm_data]))\n",
    "\n",
    "G_sig = 'GFP' if Worm == 'GFP' else 'GCaMP'\n",
    "\n",
    "### Import desired variables\n",
    "G_Raw = worm_data['G_Raw']\n",
    "R_Raw = worm_data['R_Raw']\n",
    "Time = worm_data['Time']\n",
    "proj_t = worm_data['proj_neural_thetas'].T\n",
    "print(proj_t.shape)\n",
    "print(G_Raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thanks for the update — I admit I am surprised that it does so much worse!  \n",
    "\n",
    "Can I ask how you're setting the ridge parameter (and whether it's being applied to these PCA coefficients as well as the RFP data, or just doing ML for these params)?\n",
    "\n",
    "Also, will you do one thing and just confirm that MSE is indeed *lower* on the training data?  (That is, this seems to suggest we're overfitting when adding in these regressors, but just to confirm that nothing fishy is going on it would be nice to confirm that we do get — as we'd expect — lower training error with the additional regressors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Determine which time columns have NaNs pick first and last w/o any NaNs\n",
    "badcols = np.array([x|y for (x,y) in zip(np.isnan(G_Raw).any(axis=0), np.isnan(R_Raw).any(axis=0))])\n",
    "begin_col = np.arange(0,len(Time))[~badcols][0]\n",
    "end_col = np.arange(0,len(Time))[~badcols][-1] + 1\n",
    "\n",
    "### Shave off times from before first and last full column\n",
    "G_Raw = G_Raw[:,begin_col:end_col]\n",
    "R_Raw = R_Raw[:,begin_col:end_col]\n",
    "Time = Time[begin_col:end_col, 0]\n",
    "proj_t = proj_t[:,begin_col:end_col]\n",
    "\n",
    "\n",
    "[neuron_length, neuron_time] = np.shape(G_Raw)\n",
    "print('Neurons:', neuron_length, '\\nTime Points:', neuron_time, '\\nFrom', Time[0], 's to', Time[-1], 's')\n",
    "\n",
    "### Fill in NaNs with interpolation\n",
    "for i in np.arange(len(G_Raw)):\n",
    "    \n",
    "    g_bad = np.isnan(G_Raw[i])\n",
    "    if g_bad.any():\n",
    "        g_interp = interp.interp1d(Time[~g_bad], G_Raw[i,~g_bad], kind='cubic', assume_sorted=True)\n",
    "        G_Raw[i][g_bad] = g_interp(Time[g_bad])\n",
    "    \n",
    "    r_bad = np.isnan(R_Raw[i])\n",
    "    if r_bad.any():\n",
    "        r_interp = interp.interp1d(Time[~r_bad], R_Raw[i,~r_bad], kind='cubic', assume_sorted=True)\n",
    "        R_Raw[i][r_bad] = r_interp(Time[r_bad])\n",
    "    \n",
    "    #print(i)\n",
    "    \n",
    "    ### Visualize interpolated points\n",
    "\n",
    "#     plt.scatter(Time[~r_bad], R_Raw[i,~r_bad], color='blue')\n",
    "#     plt.plot(Time, R_Raw[i], color='blue', alpha=0.2)\n",
    "#     plt.scatter(Time[r_bad], R_Raw[i,r_bad], color='red')\n",
    "#     plt.show()\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressing against all RFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define ridge regression, maximizing marginal likelihood (see Bishop p.167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define function to do ridge regression for an individual neuron\n",
    "def ridge_regress(X, Y, \n",
    "                  XTX = None, eigs = None, \n",
    "                  converge_required = 0.001, a = 1.0, b = 1.0, printer = False):\n",
    "    \n",
    "    ### Initialize\n",
    "    a_old = -1.0\n",
    "    N = len(X)\n",
    "\n",
    "    if XTX is None:\n",
    "        XTX = np.dot(X.T,X)\n",
    "\n",
    "    if eigs is None:\n",
    "        eigs = np.linalg.eigvals(XTX)\n",
    "\n",
    "    ### Loop until alpha converges\n",
    "    iterations = 0\n",
    "    while abs(a_old - a) > converge_required:\n",
    "\n",
    "        # Sigma = (b*XTX + A)^-1\n",
    "        Sigma = np.linalg.inv(b*XTX + a*np.eye(len(XTX)))\n",
    "\n",
    "        # mu = b*Sigma*X.T*Y\n",
    "        mu = b*np.dot(np.dot(Sigma, X.T), Y)\n",
    "\n",
    "        gamma = np.sum([(b*i)/(a + b*i) for i in eigs])\n",
    "        a_new = gamma/np.dot(mu.T,mu)\n",
    "\n",
    "        error = np.sum((Y - np.dot(X, mu))**2)\n",
    "        b_new = (N - gamma)/error\n",
    "\n",
    "        a_old = a\n",
    "        a = a_new\n",
    "        b = b_new\n",
    "\n",
    "        iterations += 1\n",
    "        if printer:\n",
    "            print(iterations, \"    alpha = \", a, \" beta = \", b, \" Squared-Error = \", error) \n",
    "\n",
    "    conf_int = np.sqrt(np.diag(Sigma)) # See Bishop p.167\n",
    "    \n",
    "    ### Return regression weights 'mu', std of weights 'conf_int', squared error of regression 'error'\n",
    "    return mu, conf_int, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define (Group) ARD for selecting only relevant weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GroupARD(X, Y, in_group = 1, \n",
    "             alpha_max = 100000000, max_iterations = 1000, printer=False):\n",
    "\n",
    "    N = len(X)\n",
    "    M = len(X[0])\n",
    "    \n",
    "    a = np.repeat(1.0, M) # alphas\n",
    "    b = 1.0               # Beta = 1/sig^2 \n",
    "\n",
    "    X1 = X\n",
    "\n",
    "    deletions = []\n",
    "    old_alphas = [a]\n",
    "\n",
    "    for ard_iteration in range(max_iterations):\n",
    "\n",
    "        # Sigma = (b*XTX + A)^-1\n",
    "        Sigma = np.linalg.inv(b*np.dot(X1.T,X1) + np.diag(a))\n",
    "\n",
    "        # mu = b*Sigma*X.T*Y\n",
    "        mu = b*np.dot(np.dot(Sigma, X1.T), Y)\n",
    "\n",
    "        gamma = 1.0 - a*np.diag(Sigma)\n",
    "        group_gamma = np.array([np.sum(gamma[i:i+in_group]) for i in range(0, len(gamma), in_group)])\n",
    "        mu_squared = mu**2\n",
    "        group_mu = np.array([np.sum(mu_squared[i:i+in_group]) for i in range(0, len(mu_squared), in_group)])\n",
    "        a_new = group_gamma/group_mu\n",
    "\n",
    "        error = np.sum((Y - np.dot(X1, mu))**2)\n",
    "        b_new = (N - np.sum(gamma))/error\n",
    "\n",
    "        a = [alpha for alpha in a_new for k in range(in_group)]\n",
    "        b = b_new\n",
    "\n",
    "        if printer : print(\"\\nIteration: \", ard_iteration, \" beta = \", b, \" Squared-Error = \", error)  \n",
    "\n",
    "        over = [i for i in range(len(a)) if a[i] > alpha_max]\n",
    "        if over:\n",
    "            if printer : print(\"Deletions: \", len(over))\n",
    "            deletions = [over] + deletions\n",
    "            X1 = np.delete(X1,over,axis=1)\n",
    "            a = np.delete(a,over)\n",
    "        else:\n",
    "            a_converge = np.sum((a - np.array(old_alphas[-1]))**2)\n",
    "            if printer : print(\"Alpha distance = \", a_converge, \"   Max alpha = \", np.max(a))\n",
    "            if a_converge < .00001:\n",
    "                break\n",
    "        \n",
    "        old_alphas.append(a)\n",
    "\n",
    "\n",
    "    # Recover mu\n",
    "    for i in deletions:\n",
    "        for j in i:\n",
    "            a = np.insert(a,j,-1)\n",
    "            mu = np.insert(mu,j,0)\n",
    "\n",
    "    if printer: \n",
    "        df = pd.DataFrame(list(zip(a, mu)), columns = ['alpha', 'mu'])\n",
    "        print(\"\\n\", df)\n",
    "        print(\"\\nDeletions:\", np.sum([len(i) for i in deletions])/in_group, 'out of', M/in_group)\n",
    "        \n",
    "    return mu, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate MSE for different regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MSE(R_Raw, G_Raw, Time, proj=False, proj_deg = 1, proj_t = [], \n",
    "        num_timelag=0, num_timejump=0, kfolds=5, \n",
    "        reg_type='all', alpha_max = 100000):\n",
    "\n",
    "    [neuron_length, neuron_time] = np.shape(G_Raw)\n",
    "    if proj:\n",
    "        [num_pc,_] = np.shape(proj_t)\n",
    "        print('had projections')\n",
    "\n",
    "    ### Initialize design matrix M ###\n",
    "    window = num_timelag + num_timejump + 1\n",
    "\n",
    "    # Matrix of times x neural values in window + pc projections\n",
    "    if proj:\n",
    "        M = np.zeros((neuron_time - window + 1, neuron_length*window + proj_deg * num_pc))\n",
    "        for deg in range(1,proj_deg+1):\n",
    "            for i in range(neuron_time-window+1):\n",
    "                M[i,neuron_length*window + (deg-1)*num_pc:neuron_length*window + deg*num_pc] = np.power(proj_t[:,i],deg)\n",
    "    else: \n",
    "        M = np.zeros((neuron_time - window + 1, neuron_length*window))\n",
    "    \n",
    "    for i in range(neuron_time - window + 1):\n",
    "        for j in range(neuron_length):\n",
    "            j_index = j*window\n",
    "            M[i][j_index:j_index+window] = R_Raw[j,i:i+window]\n",
    "\n",
    "\n",
    "    Y_all = G_Raw.T[num_timelag:len(G_Raw.T)-num_timejump]\n",
    "    Time_all = Time[num_timelag:len(Time)-num_timejump]\n",
    "\n",
    "    #####################################################\n",
    "\n",
    "    ### Split data into TRAIN and TEST set ###\n",
    "    kf = KFold(neuron_time-window+1, n_folds=kfolds, shuffle=False, random_state=None) \n",
    "\n",
    "    # Initalize variables\n",
    "    ARD_mu_R = None; ARD_alpha_R = None; ridge_mu_R = None; standard_mu_R = None\n",
    "    ridge_MSE_R = None; standard_MSE_R = None; ARD_MSE_R = None; single_MSE_R = None;\n",
    "    for train_index, test_index in kf:\n",
    "\n",
    "        ### Define TRAINing data\n",
    "        TRAIN_Y_all = Y_all[train_index]\n",
    "        TRAIN_M = M[train_index]\n",
    "        TRAIN_Time = Time_all[train_index]\n",
    "\n",
    "        ### Remove mean from Y\n",
    "        mean_TRAIN_Y = np.mean(TRAIN_Y_all, axis=0)\n",
    "        TRAIN_Y_all = TRAIN_Y_all - mean_TRAIN_Y\n",
    "\n",
    "        ### Define TESTing data\n",
    "        TEST_Y_all = TRAIN_Y_all#Y_all[test_index]\n",
    "        TEST_M = TRAIN_M#M[test_index]\n",
    "        TEST_Time = TRAIN_Time#Time_all[test_index]\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        ### Run regressions on TRAIN data ###\n",
    "\n",
    "        ### ARD\n",
    "        if reg_type in ['ard', 'all']:\n",
    "\n",
    "            ### Run GroupARD() on all neruons to get all weights\n",
    "            if proj:\n",
    "                ARD_mu = np.zeros((neuron_length, neuron_length*window + proj_deg*num_pc))\n",
    "                ARD_alpha = np.zeros((neuron_length, neuron_length*window + proj_deg*num_pc))\n",
    "            else:\n",
    "                ARD_mu = np.zeros((neuron_length, neuron_length*window))\n",
    "                ARD_alpha = np.zeros((neuron_length, neuron_length*window))\n",
    "            for i in range(len(ARD_mu)):\n",
    "                ARD_mu[i], ARD_alpha[i] = GroupARD(TRAIN_M, TRAIN_Y_all[:,i], in_group=window, alpha_max = alpha_max)\n",
    "            \n",
    "            # Save variables\n",
    "            if ARD_mu_R is None or ARD_alpha_R is None:\n",
    "                ARD_mu_R = ARD_mu/kfolds\n",
    "                ARD_alpha_R = ARD_alpha/kfolds\n",
    "            else:\n",
    "                ARD_mu_R = ARD_mu_R + ARD_mu/kfolds\n",
    "                ARD_alpha_R = ARD_alpha_R + ARD_alpha/kfolds\n",
    "            \n",
    "\n",
    "        ### Ridge Regression\n",
    "        if reg_type in ['ridge', 'all','ridge_std']:\n",
    "\n",
    "            ### Run ridge_regression() on all neruons to get all weights\n",
    "            if proj:\n",
    "                ridge_mu = np.zeros((neuron_length, neuron_length*window + proj_deg*num_pc))\n",
    "            else:\n",
    "                ridge_mu = np.zeros((neuron_length, neuron_length*window))\n",
    "            XTX = TRAIN_M.T @ TRAIN_M\n",
    "            eigs = np.linalg.eigvals(XTX)\n",
    "\n",
    "            for i in range(len(ridge_mu)):\n",
    "                ridge_mu[i] = ridge_regress(TRAIN_M, TRAIN_Y_all[:,i], XTX = XTX, eigs = eigs)[0]\n",
    "            if ridge_mu_R is None:\n",
    "                ridge_mu_R = ridge_mu/kfolds\n",
    "            else:\n",
    "                ridge_mu_R = ridge_mu_R + ridge_mu/kfolds\n",
    "\n",
    "        ### Standard Regression\n",
    "        if reg_type in ['standard', 'all','ridge_std']:\n",
    "            try: \n",
    "                standard_mu = np.linalg.solve(TRAIN_M.T @ TRAIN_M , TRAIN_M.T @ TRAIN_Y_all).T\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Singular matrix in standard regression, trying least squares solve\")\n",
    "                standard_mu = np.linalg.lstsq(TRAIN_M.T @ TRAIN_M , TRAIN_M.T @ TRAIN_Y_all)[0].T\n",
    "                \n",
    "            if standard_mu_R is None:\n",
    "                standard_mu_R = standard_mu/kfolds\n",
    "            else:\n",
    "                standard_mu_R = standard_mu_R + standard_mu/kfolds\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        ### Compute MSE on TEST data ###\n",
    "\n",
    "        ### Standard regression on single neuron\n",
    "        #single_MSE = np.zeros((neuron_length)); standard_MSE = None; ridge_MSE = None\n",
    "        #for i in range(neuron_length):\n",
    "        #    slope, intercept = stats.linregress(R_Raw[i,:time_index_split],G_Raw[i,:time_index_split])[:2]\n",
    "        #    single_MSE[i] = np.average((slope*R_Raw[i,time_index_split:] + intercept - G_Raw[i,time_index_split:])**2)\n",
    "        \n",
    "        #if single_MSE_R is None:\n",
    "        #    single_MSE_R = single_MSE/kfolds\n",
    "        #else:\n",
    "        #    single_MSE_R = single_MSE_R + single_MSE/kfolds\n",
    "\n",
    "        ### Ridge regression on all neurons\n",
    "        if reg_type in ['ridge', 'all', 'ridge_std']:\n",
    "            ridge_MSE = np.zeros((neuron_length))\n",
    "            for i in range(neuron_length):\n",
    "                ridge_MSE[i] = np.average(((TEST_M @ ridge_mu[i]) + mean_TRAIN_Y[i] - TEST_Y_all.T[i])**2)\n",
    "            \n",
    "            if ridge_MSE_R is None:\n",
    "                ridge_MSE_R = ridge_MSE/kfolds\n",
    "            else:\n",
    "                ridge_MSE_R = ridge_MSE_R + ridge_MSE/kfolds\n",
    "                \n",
    "        ### Standard regression on all neurons\n",
    "        if reg_type in ['standard', 'all', 'ridge_std']:\n",
    "            standard_MSE = np.zeros((neuron_length))\n",
    "            for i in range(neuron_length):\n",
    "                standard_MSE[i] = np.average(((TEST_M @ standard_mu[i]) + mean_TRAIN_Y[i] - TEST_Y_all.T[i])**2)\n",
    "\n",
    "            if standard_MSE_R is None:\n",
    "                standard_MSE_R = standard_MSE/kfolds\n",
    "            else:\n",
    "                standard_MSE_R = standard_MSE_R + standard_MSE/kfolds\n",
    "                \n",
    "        ### Group ARD on all neurons\n",
    "        if reg_type in ['ard', 'all']:\n",
    "            ARD_MSE = np.zeros((neuron_length))\n",
    "            for i in range(neuron_length):\n",
    "                ARD_MSE[i] = np.average(((TEST_M @ ARD_mu[i]) + mean_TRAIN_Y[i] - TEST_Y_all.T[i])**2)\n",
    "                \n",
    "            if ARD_MSE_R is None:\n",
    "                ARD_MSE_R = ARD_MSE/kfolds\n",
    "            else:\n",
    "                ARD_MSE_R = ARD_MSE_R + ARD_MSE/kfolds\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "    return {'ridge_mu' : ridge_mu_R, 'standard_mu' : standard_mu_R, 'ARD_mu' : ARD_mu_R,\n",
    "            'single_MSE' : single_MSE_R, 'ridge_MSE' : ridge_MSE_R, 'standard_MSE' : standard_MSE_R, 'ARD_MSE' : ARD_MSE_R,\n",
    "            #'M' : M, 'Y_all' : Y_all, 'Time_all' : Time_all, \n",
    "            #'time_index_split' : time_index_split, 'mean_TRAIN_Y' : mean_TRAIN_Y,\n",
    "            'ARD_alpha' : ARD_alpha_R}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Unpack output from MSE()\n",
    "outProj1 = MSE(R_Raw,G_Raw,Time,proj = True, proj_t = proj_t, proj_deg = 1, reg_type='ridge_std',num_timelag=0, alpha_max=10000)\n",
    "print(\"ridge_MSE %3.3f, standard_MSE %3.3f\" %(np.mean(outProj1['ridge_MSE']), np.mean(outProj1['standard_MSE'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outProj2 = MSE(R_Raw,G_Raw,Time,proj = True, proj_t = proj_t, proj_deg = 2, reg_type='ridge_std',num_timelag=0, alpha_max=10000)\n",
    "print(\"ridge_MSE %3.3f, standard_MSE %3.3f\" %(np.mean(outProj2['ridge_MSE']), np.mean(outProj2['standard_MSE'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = MSE(R_Raw,G_Raw,Time,proj = False, proj_t = proj_t, proj_deg = 2, reg_type='ridge_std',num_timelag=0, alpha_max=10000)\n",
    "print(\"ridge_MSE %3.3f, standard_MSE %3.3f\" %(np.mean(out['ridge_MSE']), np.mean(out['standard_MSE'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = MSE(R_Raw,G_Raw,Time,proj = False, proj_t = proj_t, proj_deg = 2, reg_type='ridge_std',num_timelag=0, alpha_max=10000)\n",
    "print(\"ridge_MSE %3.3f, standard_MSE %3.3f\" %(np.mean(out['ridge_MSE']), np.mean(out['standard_MSE'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,10))\n",
    "plt.plot(out['ridge_MSE'])\n",
    "plt.plot(outProj1['ridge_MSE'])\n",
    "plt.plot(outProj2['ridge_MSE'])\n",
    "plt.xlabel('Neurons', size=14)\n",
    "plt.ylabel('MSE',size=14)\n",
    "plt.title('Ridge Regression',size=20)\n",
    "plt.legend(['Neural activity','Neural activity and projections (deg 1)', 'Neural activity and projections (deg 2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,10))\n",
    "plt.plot(out['standard_MSE'])\n",
    "plt.plot(outProj1['standard_MSE'])\n",
    "plt.plot(outProj2['standard_MSE'])\n",
    "plt.xlabel('Neurons', size=14)\n",
    "plt.ylabel('MSE',size=14)\n",
    "plt.title('Standard Regression',size=20)\n",
    "plt.legend(['Neural activity','Neural activity and projections (deg 1)', 'Neural activity and projections (deg 2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ridge_MSE/standard_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking at how many timesteps back to take. \n",
    "results = []\n",
    "errors = []\n",
    "for i in range(3):\n",
    "    temp = MSE(R_Raw,G_Raw,Time, proj_t, alpha_max=100000, num_timelag=i, num_timejump=i)\n",
    "    results.append(temp)\n",
    "    errors.append(temp['ridge_MSE']/temp['single_MSE'])\n",
    "    \n",
    "print(pd.DataFrame(np.array(errors).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(errors[0], bins=60, range = (0.5,3.5), color = 'red', alpha=0.5, cumulative=True)\n",
    "plt.hist(errors[1], bins=60, range = (0.5,3.5), color = 'green', alpha=0.5, cumulative=True)\n",
    "plt.hist(errors[4], bins=60, range = (0.5,3.5), color = 'blue', alpha=0.5, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(errors)):\n",
    "    print(i, np.mean(errors[i]), np.median(errors[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(standard_mu[0], color = 'blue')\n",
    "plt.plot(ridge_mu[0], color = 'green')\n",
    "plt.plot(ARD_mu[0], color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Look at distribution of NaNs in original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get list of columns with any NaN present\n",
    "badcols = np.array([x|y for (x,y) in zip(np.isnan(G_Raw).any(axis=0), np.isnan(R_Raw).any(axis=0))])\n",
    "\n",
    "### NaN Distribution Information\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(len(np.nonzero(badcols)[0]), 'columns/times containing NaNs were found')\n",
    "\n",
    "print('\\nNaN Count by Time Column, Out of', len(G_Raw), 'Neurons')\n",
    "print(pd.DataFrame(np.column_stack((Time[badcols],np.sum(np.isnan(G_Raw[:,badcols]), axis=0),np.sum(np.isnan(R_Raw[:,badcols]), axis=0))), \n",
    "                   index = np.nonzero(badcols)[0], columns=['Time', 'G_Raw', 'R_Raw']))\n",
    "\n",
    "print('\\nNaN Count by Neruon, Out of', len(G_Raw[0]), 'Time Points')\n",
    "print(pd.DataFrame(np.column_stack((np.sum(np.isnan(G_Raw), axis=1),np.sum(np.isnan(R_Raw), axis=1))), \n",
    "                   columns=['G_Raw', 'R_Raw']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove all columns containing any NaN, instead of interpolating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Remove Columns containing NaNs\n",
    "badcols = np.array([x|y for (x,y) in zip(np.isnan(G_Raw).any(axis=0), np.isnan(R_Raw).any(axis=0))])\n",
    "\n",
    "G_Raw = G_Raw[:,~badcols]\n",
    "R_Raw = R_Raw[:,~badcols]\n",
    "Time = Time[~badcols]\n",
    "\n",
    "[neuron_length, neuron_time] = np.shape(G_Raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a heat map of GFP/GCaMP & RFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Heat Map of GFP/GCaMP & RFP across trial\n",
    "f ,ax = plt.subplots(1,2, figsize=(16,10*2))\n",
    "ax[0].imshow(G_Raw, aspect=10,cmap=\"Paired\")\n",
    "ax[1].imshow(R_Raw, aspect=10,cmap=\"Paired\")\n",
    "for axis, title in zip(ax, [G_sig, r'RFP']):\n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel(r'Frame #')\n",
    "    axis.set_ylabel(r'Neuron #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examine Raw Fluorescence statistics within worm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize distribution of Raw fluorescence statistics across neurons\n",
    "p = 8 # fluorescence percentile\n",
    "\n",
    "f, ax = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "ax[0][0].hist(np.min(G_Raw, axis=1), color='green')\n",
    "ax[0][1].hist(np.percentile(G_Raw, p, axis=1), color='green')\n",
    "ax[0][2].hist(np.median(G_Raw, axis=1), color='green')\n",
    "ax[0][3].hist(np.max(G_Raw, axis=1), color='green')\n",
    "\n",
    "ax[1][0].hist(np.min(R_Raw, axis=1), color='red')\n",
    "ax[1][1].hist(np.percentile(R_Raw, p, axis=1), color='red')\n",
    "ax[1][2].hist(np.median(R_Raw, axis=1), color='red')\n",
    "ax[1][3].hist(np.max(R_Raw, axis=1), color='red')\n",
    "\n",
    "ax[0][0].set_title(G_sig + ', Minimum Raw', fontsize = 16)\n",
    "ax[0][1].set_title(G_sig + ', ' + str(p) + 'th Percentile Raw', fontsize = 16)\n",
    "ax[0][2].set_title(G_sig + ', Median Raw', fontsize = 16)\n",
    "ax[0][3].set_title(G_sig + ', Maximum Raw', fontsize = 16)\n",
    "ax[1][0].set_title('RFP, Minimum Raw', fontsize = 16)\n",
    "ax[1][1].set_title('RFP, ' + str(p) + 'th Percentile Raw', fontsize = 16)\n",
    "ax[1][2].set_title('RFP, Median Raw', fontsize = 16)\n",
    "ax[1][3].set_title('RFP, Maximum Raw', fontsize = 16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Data/Worm'+Worm+'Raw_Fluorescence_Dist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize raw activity (optionally rescaled) of particular neruons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize Raw Activity of specific neruon (optionally rescaled to 0-1)\n",
    "def rescale(a):\n",
    "    return [(i-np.min(a))/(np.max(a)-np.min(a)) for i in a]\n",
    "\n",
    "n = 12\n",
    "f = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(Time, rescale(G_Raw[n]), color = 'green')\n",
    "plt.plot(Time, rescale(R_Raw[n]), color = 'red')\n",
    "\n",
    "# plt.plot(Time, G_Raw[n], color = 'green')\n",
    "# plt.plot(Time, R_Raw[n], color = 'red')\n",
    "\n",
    "plt.xlim([0,Time[-1]])\n",
    "\n",
    "plt.show()\n",
    "print('Neuron: ', n)\n",
    "print('            Min       %  Median     Max')\n",
    "print('G_Raw: ', '%7.1f' % np.min(G_Raw[n]), '%7.1f' % np.percentile(G_Raw[n],8), '%7.1f' % np.median(G_Raw[n]), '%7.1f' % np.max(G_Raw[n]))\n",
    "print('R_Raw: ', '%7.1f' % np.min(R_Raw[n]), '%7.1f' % np.percentile(R_Raw[n],8), '%7.1f' % np.median(R_Raw[n]), '%7.1f' % np.max(R_Raw[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do basic linear regression of GFP/GCaMP against RFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Caclualte regression info of GFP/GCaMP against RFP\n",
    "slopes = np.zeros(neuron_length)\n",
    "r_values = np.zeros(neuron_length)\n",
    "intercepts = np.zeros(neuron_length)\n",
    "\n",
    "for i in range(neuron_length):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(R_Raw[i],G_Raw[i])\n",
    "    slopes[i] = slope\n",
    "    r_values[i] = r_value\n",
    "    intercepts[i] = intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Coefficeint of Varaiation figure for RFP vs GFP w/ r^2 as colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Display Coefficeint of Variation for RFP vs GFP w/ r^2 as colorbar\n",
    "def onpick3(event):\n",
    "    ind = event.ind\n",
    "    print('onpick3 scatter:', ind)\n",
    "\n",
    "f = plt.figure(figsize=(10,10))\n",
    "plt.scatter(stats.variation(R_Raw, axis=1),stats.variation(G_Raw, axis=1), c = r_values, cmap='hot', picker=True)\n",
    "plt.scatter(np.var(R_Raw, axis=1),np.var(G_Raw, axis=1), c = r_values, cmap='hot', picker=True)\n",
    "plt.plot(np.arange(0.01,0.6,.01),np.arange(0.01,0.6,.01))\n",
    "plt.xlim([0,.6])\n",
    "plt.ylim([0,.6])\n",
    "plt.xlabel('Cof. of Var. RFP Raw')\n",
    "plt.ylabel('Cof. of Var. GFP Raw')\n",
    "plt.colorbar(label=r'$r^2$')\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(stats.variation(R_Raw, axis=1),stats.variation(G_Raw, axis=1))\n",
    "print(slope, intercept, r_value**2, p_value, std_err)\n",
    "plt.plot(np.arange(0.01,0.6,.01), intercept + slope*np.arange(0.01,0.6,.01), 'g--')\n",
    "\n",
    "f.canvas.mpl_connect('pick_event', onpick3)\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('Coeffcient_of_Variation.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize fit of weights to neural activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define which weights to use & neruons to see\n",
    "weights = ridge_mu\n",
    "neuron_n = [52,55,12,42]  # List of neurons to be observed\n",
    "\n",
    "f ,ax = plt.subplots(len(neuron_n),2, figsize=(16,4*len(neuron_n)))\n",
    "for i,j in enumerate(neuron_n):\n",
    "#     ax[i][0].plot(Time_all, Y_all.T[j]) # Actual activity\n",
    "#     ax[i][0].plot(Time_all, (M @ weights[j])) # Recovered Activity using Estimated Weights\n",
    "#     ax[i][1].plot(Time_all, (M @ weights[j]) - Y_all.T[j]) # Error\n",
    "    \n",
    "    ax[i][0].plot(Time_all[time_index_split:], Y_all.T[j,time_index_split:]) # Actual activity\n",
    "    ax[i][0].plot(Time_all[time_index_split:], (M @ weights[j] + mean_TRAIN_Y[j])[time_index_split:]) # Recovered Activity using Estimated Weights\n",
    "    #ax[i][0].plot(Time_all[time_index_split:], (M @ ARD_mu[j] + mean_TRAIN_Y[j])[time_index_split:]) # Recovered Activity using Estimated Weights\n",
    "    ax[i][1].plot(Time_all[time_index_split:], ((M @ weights[j]) - Y_all.T[j] + mean_TRAIN_Y[j])[time_index_split:]) # Error\n",
    "    \n",
    "    ax[i][0].set_ylabel(r'Neuron ' + str(neuron_n[i]))\n",
    "    \n",
    "for axis, title in zip(ax[0], [r'Actual Activity & Estimated Activity', r'Error']):\n",
    "    axis.set_title(title)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define which weights to use & neruons to see\n",
    "weights = ridge_mu\n",
    "neuron_n = [12,42]  # List of neurons to be observed\n",
    "\n",
    "f ,ax = plt.subplots(len(neuron_n),1, figsize=(12,4*len(neuron_n)))\n",
    "for i,j in enumerate(neuron_n):\n",
    "#     ax[i][0].plot(Time_all, Y_all.T[j]) # Actual activity\n",
    "#     ax[i][0].plot(Time_all, (M @ weights[j])) # Recovered Activity using Estimated Weights\n",
    "#     ax[i][1].plot(Time_all, (M @ weights[j]) - Y_all.T[j]) # Error\n",
    "    \n",
    "    ax[i].plot(Time_all[time_index_split:], Y_all.T[j,time_index_split:], label='Actual Activity') # Actual activity\n",
    "    ax[i].plot(Time_all[time_index_split:], (M @ weights[j] + mean_TRAIN_Y[j])[time_index_split:], label='Predicted Activity') # Recovered Activity using Estimated Weights\n",
    " \n",
    "    ax[i].set_title(r'Neuron #' + str(neuron_n[i]))\n",
    "    ax[i].set_xlabel(r'Time (s)')\n",
    "    ax[i].set_ylabel(r'Raw GFP Fluorescence')\n",
    "    ax[i].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
